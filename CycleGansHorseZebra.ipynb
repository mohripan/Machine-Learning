{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CycleGansHorseZebra.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNwd3g5GvKAu+qOhVzGpFLY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohripan/Machine-Learning/blob/main/CycleGansHorseZebra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDR810yk7UZ7",
        "outputId": "47661633-7fc8-4d60-bb16-9f9e19a7082a"
      },
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps albumentations\n",
        "!pip install qudida"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting albumentations\n",
            "  Using cached albumentations-1.1.0-py3-none-any.whl (102 kB)\n",
            "Installing collected packages: albumentations\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 1.1.0\n",
            "    Uninstalling albumentations-1.1.0:\n",
            "      Successfully uninstalled albumentations-1.1.0\n",
            "Successfully installed albumentations-1.1.0\n",
            "Requirement already satisfied: qudida in /usr/local/lib/python3.7/dist-packages (0.0.4)\n",
            "Requirement already satisfied: numpy>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from qudida) (1.19.5)\n",
            "Requirement already satisfied: opencv-python-headless>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from qudida) (4.5.4.58)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida) (0.22.2.post1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from qudida) (3.7.4.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QheFCUB7yDbB"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile, requests, io\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6VnlyWryavd"
      },
      "source": [
        "zip_url = 'https://storage.googleapis.com/kaggle-data-sets/210524/459209/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20211031%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20211031T095629Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=47bde570c2a95274e1b567277b186d50bd079afce8ab6d222b646bcf296a0281647af24e4216476787171bfd03c18f1c3b31fe42ec9c064f36e862e526774fd8134827e31b555763a84f89df025d5c74f022c2cfd635019320c7e00655326c610c0be98a214f65d859c0d36c80772d705d9857f1b9c8d3f53750be3c8500711186624596157b6a73360d8f38454a7054db639d0c289458967b2f061f48639266afa746df8c48679fe76dedc579c835d3a1ee9f6a8f2d6b1fffd84c8d7b6770e19b132767497ba97f385c0c67c7c5c21fa76a19be7a5feadb4bef4e8a6e10aae7b997a4741530f536fa035533d553a42cf3ea178cdd37d6c7c970a005b37a867e'\n",
        "r = requests.get(zip_url)\n",
        "with zipfile.ZipFile(io.BytesIO(r.content)) as my_zip:\n",
        "  my_zip.extractall()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M1h-VGSzJYg"
      },
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, stride):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Sequential(nn.Conv2d(in_channels, out_channels, 4, stride, 1, bias=True, padding_mode='reflect'),\n",
        "                              nn.InstanceNorm2d(out_channels),\n",
        "                              nn.LeakyReLU(0.2))\n",
        "      \n",
        "  def forward(self, x):\n",
        "    return self.conv(x)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG6Yw13G0FPH"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n",
        "    super().__init__()\n",
        "    self.initial = nn.Sequential(nn.Conv2d(in_channels,\n",
        "                                           features[0],\n",
        "                                           4, 2, 1,\n",
        "                                           padding_mode='reflect'),\n",
        "                                 nn.LeakyReLU(0.2),)\n",
        "    \n",
        "    layers = []\n",
        "    in_channels = features[0]\n",
        "\n",
        "    for feature in features[1:]:\n",
        "      layers.append(Block(in_channels, feature, stride=1 if feature==features[-1] else 2))\n",
        "      in_channels = feature\n",
        "\n",
        "    layers.append(nn.Conv2d(in_channels, 1, 4, 1, 1, padding_mode='reflect'))\n",
        "    self.mode = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.initial(x)\n",
        "    return torch.sigmoid(self.mode(x))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1P6Bgn11eYJ"
      },
      "source": [
        "class ConvBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, down=True, use_act=True, **kwargs):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, padding_mode='reflect', **kwargs)\n",
        "        if down\n",
        "        else nn.ConvTranspose2d(in_channels, out_channels, **kwargs),\n",
        "        nn.InstanceNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True) if use_act else nn.Identity()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_ZqHYXi1qOO"
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self, channels):\n",
        "    super().__init__()\n",
        "    self.block = nn.Sequential(ConvBlock(channels, channels, kernel_size=3, padding=1),\n",
        "                               ConvBlock(channels, channels, use_act=False, kernel_size=3, padding=1),\n",
        "                               )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x+self.block(x)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXTG8WnC26ha"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, img_channels, num_features=64, num_residuals=9):\n",
        "    super().__init__()\n",
        "    self.initial = nn.Sequential(\n",
        "        nn.Conv2d(img_channels, num_features, kernel_size=7, stride=1, padding=3, padding_mode='reflect'),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "    self.down_blocks = nn.ModuleList(\n",
        "        [\n",
        "         ConvBlock(num_features, num_features*2, kernel_size=3, stride=2, padding=1),\n",
        "         ConvBlock(num_features*2, num_features*4, kernel_size=3, stride=2, padding=1),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    self.residual_blocks = nn.Sequential(\n",
        "        *[ResidualBlock(num_features*4) for _ in range(num_residuals)]\n",
        "    )\n",
        "\n",
        "    self.up_blocks = nn.ModuleList(\n",
        "        [\n",
        "         ConvBlock(num_features*4, num_features*2, down=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "         ConvBlock(num_features*2, num_features, down=False, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    self.last = nn.Conv2d(num_features, img_channels, kernel_size=7, stride=1, padding=3, padding_mode='reflect')\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.initial(x)\n",
        "    for layer in self.down_blocks:\n",
        "      x = layer(x)\n",
        "    x = self.residual_blocks(x)\n",
        "    for layer in self.up_blocks:\n",
        "      x = layer(x)\n",
        "    \n",
        "    return torch.tanh(self.last(x))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk4ibMbR6tVV",
        "outputId": "29f5eec6-7835-491c-f3c2-18b983a9c77b"
      },
      "source": [
        "!pip install config"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: config in /usr/local/lib/python3.7/dist-packages (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkTyFHaK4FC3"
      },
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import config"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stKJsvaf7DCN"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_horse_dir = '/content/horse2zebra/horse2zebra/trainA'\n",
        "train_zebra_dir = '/content/horse2zebra/horse2zebra/trainB'\n",
        "val_horse_dir = '/content/horse2zebra/horse2zebra/testA'\n",
        "val_zebra_dir = '/content/horse2zebra/horse2zebra/testB'\n",
        "\n",
        "batch_size = 1\n",
        "lr = 1e-5\n",
        "lambda_identity = 0.0\n",
        "lambda_cycle = 10\n",
        "num_workers = 4\n",
        "num_epochs = 10\n",
        "load_model = False\n",
        "save_model = True\n",
        "checkpoint_gen_H = 'genh.pth.tar'\n",
        "checkpoint_gen_Z = 'genz.pth.tar'\n",
        "checkpoint_critic_H = 'critich.pth.tar'\n",
        "checkpoint_critic_Z = 'criticz.pth.tar'\n",
        "\n",
        "transforms = A.Compose([\n",
        "                        A.Resize(256, 256),\n",
        "                        A.HorizontalFlip(0.5),\n",
        "                        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255),\n",
        "                        ToTensorV2(),\n",
        "], additional_targets={'image0':'image'})"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDwZSflr5XbB"
      },
      "source": [
        "def save_checkpoint(model, optimizer, filename='horse_zebra.pth.tar'):\n",
        "  print('=> Saving checkpoint')\n",
        "  checkpoint = {\n",
        "      'state_dict': model.state_dict(),\n",
        "      'optimizer': optimizer.state_dict(),\n",
        "  }\n",
        "  torch.save(checkpoint, filename)\n",
        "\n",
        "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
        "  print('=> Loading checkpoint')\n",
        "  checkpoint = torch.load(checkpoint_file, map_location=device)\n",
        "  model.load_state_dict(checkpoint['state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = lr"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDEm7LpOJzzP"
      },
      "source": [
        "import random"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rUsOnJ8J06T"
      },
      "source": [
        "def seed_everything(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7al3ql0Y9UcH"
      },
      "source": [
        "class HorseZebraDataset(Dataset):\n",
        "  def __init__(self, root_zebra, root_horse, transform=None):\n",
        "    self.root_zebra = root_zebra\n",
        "    self.root_horse = root_horse\n",
        "    self.transform = transform\n",
        "\n",
        "    self.zebra_images = os.listdir(root_zebra)\n",
        "    self.horse_images = os.listdir(root_horse)\n",
        "    self.length_dataset = max(len(self.zebra_images), len(self.horse_images))\n",
        "    self.zebra_len = len(self.zebra_images)\n",
        "    self.horse_len = len(self.horse_images)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length_dataset\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    zebra_img = self.zebra_images[idx % self.zebra_len]\n",
        "    horse_img = self.horse_images[idx % self.horse_len]\n",
        "\n",
        "    zebra_path = os.path.join(self.root_zebra, zebra_img)\n",
        "    horse_path = os.path.join(self.root_horse, horse_img)\n",
        "\n",
        "    zebra_img = np.array(Image.open(zebra_path).convert('RGB'))\n",
        "    horse_img = np.array(Image.open(horse_path).convert('RGB'))\n",
        "\n",
        "    if self.transform:\n",
        "      augmentation = self.transform(image=zebra_img, image0=horse_img)\n",
        "      zebra_img = augmentation['image']\n",
        "      horse_img = augmentation['image0']\n",
        "\n",
        "    return zebra_img, horse_img"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91oADOUHJqE2"
      },
      "source": [
        "from tqdm import tqdm\n",
        "from torchvision.utils import save_image"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taVbWBr5Khoq"
      },
      "source": [
        "def train_fn(disc_H, disc_Z, gen_Z, gen_H, loader, opt_disc, opt_gen, L1, mse, d_scaler, g_scaler):\n",
        "  loop = tqdm(loader, leave=True)\n",
        "\n",
        "  for idx, (zebra, horse) in enumerate(loop):\n",
        "    zebra, horse = zebra.to(device), horse.to(device)\n",
        "\n",
        "    with torch.cuda.amp.autocast():\n",
        "      fake_horse = gen_H(zebra)\n",
        "      D_H_real = disc_H(horse)\n",
        "      D_H_fake = disc_H(fake_horse.detach())\n",
        "      D_H_real_loss = mse(D_H_real, torch.ones_like(D_H_real))\n",
        "      D_H_fake_loss = mse(D_H_fake, torch.zeros_like(D_H_fake))\n",
        "      D_H_loss = D_H_real_loss+D_H_fake_loss\n",
        "\n",
        "      fake_zebra = gen_Z(horse)\n",
        "      D_Z_real = disc_Z(zebra)\n",
        "      D_Z_fake = disc_Z(fake_zebra.detach())\n",
        "      D_Z_real_loss = mse(D_Z_real, torch.ones_like(D_Z_real))\n",
        "      D_Z_fake_loss = mse(D_Z_fake, torch.zeros_like(D_Z_fake))\n",
        "      D_Z_loss = D_Z_real_loss+D_Z_fake_loss\n",
        "\n",
        "      D_loss = (D_H_loss+D_Z_loss)/2\n",
        "\n",
        "    opt_disc.zero_grad()\n",
        "    d_scaler.scale(D_loss).backward()\n",
        "    d_scaler.step(opt_disc)\n",
        "    d_scaler.update()\n",
        "\n",
        "    with torch.cuda.amp.autocast():\n",
        "      D_H_fake = disc_H(fake_horse)\n",
        "      D_Z_fake = disc_Z(fake_zebra)\n",
        "      loss_G_H = mse(D_H_fake, torch.ones_like(D_H_fake))\n",
        "      loss_G_Z = mse(D_Z_fake, torch.ones_like(D_Z_fake))\n",
        "\n",
        "      cycle_zebra = gen_Z(fake_horse)\n",
        "      cycle_horse = gen_H(fake_zebra)\n",
        "      cycle_zebra_loss = L1(zebra, cycle_zebra)\n",
        "      cycle_horse_loss = L1(horse, cycle_horse)\n",
        "\n",
        "      identity_zebra = gen_Z(zebra)\n",
        "      identity_horse = gen_H(horse)\n",
        "      identity_zebra_loss = L1(zebra, identity_zebra)\n",
        "      identity_horse_loss = L1(horse, identity_horse)\n",
        "\n",
        "      G_loss = (loss_G_Z+\n",
        "                loss_G_H+\n",
        "                cycle_zebra_loss*lambda_cycle+\n",
        "                cycle_horse_loss*lambda_cycle+\n",
        "                identity_horse_loss*lambda_identity+\n",
        "                identity_zebra_loss*lambda_identity)\n",
        "      \n",
        "      opt_gen.zero_grad()\n",
        "      g_scaler.scale(G_loss).backward()\n",
        "      g_scaler.step(opt_gen)\n",
        "      g_scaler.update()\n",
        "\n",
        "      if idx%200 == 0:\n",
        "        save_image(fake_horse*0.5+0.5, f'saved_images/horse_{idx}.png')\n",
        "        save_image(fake_zebra*0.5+0.5, f'saved_images/zebra_{idx}.png')\n",
        "  \n",
        "\n",
        "def main():\n",
        "  disc_H = Discriminator(in_channels=3).to(device)\n",
        "  disc_Z = Discriminator(in_channels=3).to(device)\n",
        "  gen_Z = Generator(img_channels=3, num_residuals=9).to(device)\n",
        "  gen_H = Generator(img_channels=3, num_residuals=9).to(device)\n",
        "\n",
        "  opt_disc = optim.Adam(list(disc_H.parameters()) + list(disc_Z.parameters()), lr=lr, betas=(0.5, 0.999),)\n",
        "\n",
        "  opt_gen = optim.Adam(list(gen_Z.parameters()) + list(gen_H.parameters()), lr=lr, betas=(0.5, 0.999),)\n",
        "\n",
        "  L1 = nn.L1Loss()\n",
        "  mse = nn.MSELoss()\n",
        "\n",
        "  if load_model:\n",
        "    load_checkpoint(checkpoint_gen_H, gen_H, opt_gen, lr)\n",
        "    load_checkpoint(checkpoint_gen_Z, gen_Z, opt_gen, lr)\n",
        "    load_checkpoint(checkpoint_critic_H, disc_H, opt_disc, lr)\n",
        "    load_checkpoint(checkpoint_critic_Z, disc_Z, opt_disc, lr)\n",
        "\n",
        "  dataset = HorseZebraDataset(root_horse=train_horse_dir, root_zebra=train_zebra_dir, transform=transforms)\n",
        "\n",
        "  loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "  g_scaler = torch.cuda.amp.GradScaler()\n",
        "  d_scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "  for i in range(num_epochs):\n",
        "    train_fn(disc_H, disc_Z, gen_Z, gen_H, loader, opt_disc, opt_gen, L1, mse, d_scaler, g_scaler)\n",
        "\n",
        "    if save_model:\n",
        "      save_checkpoint(gen_H, opt_gen, filename=checkpoint_gen_H)\n",
        "      save_checkpoint(gen_Z, opt_gen, filename=checkpoint_gen_Z)\n",
        "      save_checkpoint(disc_H, opt_disc, filename=checkpoint_critic_H)\n",
        "      save_checkpoint(disc_Z, opt_disc, filename=checkpoint_critic_Z)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oqI4kRAL76R",
        "outputId": "4fbf872c-0a7c-4c1c-ab93-f450831b6ce0"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 1334/1334 [54:15<00:00,  2.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 62%|██████▏   | 831/1334 [33:50<20:27,  2.44s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyh9ovotSkIZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}